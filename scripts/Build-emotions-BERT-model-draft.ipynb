{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "BERT_PATH = '../data/bert'\n",
    "MODEL_PATH = \"bert_model.bin\"\n",
    "TRAINING_FILE = \"bert_train.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (76340, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "SENTENCE_EMOTIONS_TEST_FILE = '../data/sentence_emotions_test.pickle' \n",
    "SENTENCE_EMOTIONS_TRAIN_FILE = '../data/sentence_emotions_train.pickle'\n",
    "CONTINUES_TO_BINARY_THRESHOLD = 0.5\n",
    "\n",
    "from limbic.limbic_constants import AFFECT_INTENSITY_EMOTIONS as EMOTIONS\n",
    "\n",
    "\n",
    "def load_data_file(file_path):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    \n",
    "    data_sentences = data['text'] #.str.lower().apply(lambda x: utils.preprocess_sentence(x))\n",
    "    y_data = data[EMOTIONS].values\n",
    "\n",
    "    #     # This will be used throughout the notebook to compute performance \n",
    "#     y_data_labeled = utils.continuous_labels_to_binary(y_data, CONTINUES_TO_BINARY_THRESHOLD)   \n",
    "\n",
    "#     # This representation will be needed for sklearn later in this notebook. \n",
    "#     x_data = tokenizer.texts_to_sequences(data_sentences)\n",
    "#     x_data = tf.keras.preprocessing.sequence.pad_sequences(x_data, maxlen=MAX_LEN)\n",
    "\n",
    "#     return data, x_data, y_data, y_data_labeled, data_sentences\n",
    "    return data, y_data, data_sentences\n",
    "\n",
    "\n",
    "train, y_train_labeled, train_sentences = load_data_file(SENTENCE_EMOTIONS_TRAIN_FILE)\n",
    "# test, x_test, y_test, y_test_labeled, test_sentences = load_data_file(SENTENCE_EMOTIONS_TEST_FILE)\n",
    "\n",
    "print(f'train shape: {train.shape}')\n",
    "# print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.406, 0.206],\n",
       "       ...,\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[EMOTIONS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class BERTDataset:\n",
    "    def __init__(self, text, target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "        )\n",
    "        \n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, NUM_LABELS)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o2)\n",
    "        output = nn.functional.softmax(self.out(bo), dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Tensorboard + PyTorch: https://towardsdatascience.com/a-complete-guide-to-using-tensorboard-with-pytorch-53cb2301e8c3\n",
    "TB_WRITER = SummaryWriter()\n",
    "\n",
    "def loss_fn(outputs, targets, num_labels):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, num_labels))\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler, num_labels, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = d['ids']\n",
    "        token_type_ids = d['token_type_ids']\n",
    "        mask = d['mask']\n",
    "        targets = d['targets']\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets, num_labels)\n",
    "        total_loss += loss\n",
    "        n += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    TB_WRITER.add_scalar(\"Loss/train\", total_loss/n, epoch)\n",
    "\n",
    "        \n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "#             fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "NUM_LABELS = 4\n",
    "\n",
    "\n",
    "def run():\n",
    "#     dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "#     dfx.sentiment = dfx.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    dfx, y_train_labeled, train_sentences = load_data_file(SENTENCE_EMOTIONS_TRAIN_FILE)\n",
    "\n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx.sample(n=100, random_state=1), test_size=0.1, random_state=42, #stratify=dfx.sentiment.values # Consider iterative stratified\n",
    "    )\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BERTDataset(\n",
    "        text=df_train.text.values, target=df_train[EMOTIONS].values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDataset(\n",
    "        text=df_valid.text.values, target=df_valid[EMOTIONS].values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n",
    "    device = torch.device(DEVICE)\n",
    "    model = BERTBaseUncased()\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_accuracy = 0\n",
    "    for epoch in tqdm(range(EPOCHS), 'epochs'):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler, NUM_LABELS, epoch)\n",
    "        outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0f937dca74cca9dd51896ee33ecd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9743940a5fab4f4d8fdb263aeea91f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d19b650b8514be3ba8b18156301f7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.7916666666666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa05ba4abf934eacba6c591bda3b726c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcb398227594e1ca21702f28772020e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.8666666666666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35497202d7be41b093493c3ed6aac1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52deff5ee7f4ceb87669febc4caea24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a7405230f942ac8eb9f990c9866de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0bcdb44ac44e5ca9f11d0be0a3a17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c86fc00747c4c43adde8cd0154918b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e0f56a12754efc9f417a8492ba89d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f6231c445d40548f6d34e4de5d7653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e255456a65b44d49c4f38925b719a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d30d877aafe43f98cebb4a08cffb144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7aebf76fb584173970054c8bbb67534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6804ec44cc2d4e18a4f66f45d1e55f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540f7c16e40a4afab323d5da68c1be6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056b3688399846b08801d4aed87283b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e3e9e12c0341e2861c49acc22da846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb838567d20458c8b2c0f3f794c14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6991c3a12b58493f8c1520a3b78ec99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Avg Precision Score = 0.9083333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print('training....')\n",
    "\n",
    "dfx, y_train_labeled, train_sentences = load_data_file(SENTENCE_EMOTIONS_TRAIN_FILE)\n",
    "\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    dfx.sample(n=100, random_state=1), test_size=0.1, random_state=42, #stratify=dfx.sentiment.values # Consider iterative stratified\n",
    ")\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "train_dataset = BERTDataset(\n",
    "    text=df_train.text.values, target=df_train[EMOTIONS].values\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4\n",
    ")\n",
    "\n",
    "valid_dataset = BERTDataset(\n",
    "    text=df_valid.text.values, target=df_valid[EMOTIONS].values\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    ")\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "model = BERTBaseUncased()\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "best_accuracy = 0\n",
    "for epoch in tqdm(range(EPOCHS), 'epochs'):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler, NUM_LABELS, epoch)\n",
    "    outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "    targets = np.array(np.array(targets) >= 0.1).astype(int)\n",
    "    outputs = np.array(outputs) #>= 0.5\n",
    "    accuracy = label_ranking_average_precision_score(targets, outputs)\n",
    "    print(f\"Ranking Avg Precision Score = {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "TB_WRITER.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.array(targets)\n",
    "# label_ranking_average_precision_score(np.array(targets).as, np.array(outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = np.array([[0.0, 1.0, 0.0, 0.0],\n",
    "#  [0, 1, 0, 0]])\n",
    "# y_score = np.array([[0.5 , 0.5, 0.5, 0.5],\n",
    "#        [0.5, 0.5, 0.5, 0.5]])\n",
    "# label_ranking_average_precision_score(y_true, y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = np.array(targets) >= 0.5\n",
    "# t.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(np.array(targets) >= 0.1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTBaseUncased(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTION_DICT = dict()\n",
    "import time\n",
    "\n",
    "\n",
    "def sentence_prediction(sentence):\n",
    "    tokenizer = TOKENIZER\n",
    "    max_len = MAX_LEN\n",
    "    review = str(sentence)\n",
    "    review = \" \".join(review.split())\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        review, None, add_special_tokens=True, max_length=max_len)\n",
    "\n",
    "    ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "    padding_length = max_len - len(ids)\n",
    "    ids = ids + ([0] * padding_length)\n",
    "    mask = mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    ids = ids.to(DEVICE, dtype=torch.long)\n",
    "    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "    mask = mask.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "    outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "    return outputs\n",
    "#     outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "#     return outputs[0][0]\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    start_time = time.time()\n",
    "    positive_prediction = sentence_prediction(sentence)\n",
    "\n",
    "    response = {}\n",
    "    response[\"response\"] = {\n",
    "        \"prediction\": positive_prediction,\n",
    "        \"sentence\": str(sentence),\n",
    "        \"time_taken\": str(time.time() - start_time),\n",
    "    }\n",
    "    return response\n",
    "\n",
    "MODEL = BERTBaseUncased()\n",
    "MODEL.load_state_dict(torch.load(MODEL_PATH))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'prediction': tensor([[0.1787, 0.5970, 0.1025, 0.1218]], grad_fn=<SoftmaxBackward>),\n",
       "  'sentence': 'this is a sentence with hate',\n",
       "  'time_taken': '0.24013590812683105'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = nn.functional.softmax(self.out(bo), dim=1)\n",
    "p = predict('this is a sentence with hate')\n",
    "p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17873068, 0.5970073 , 0.10250984, 0.1217522 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['response']['prediction'].detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'fear', 'anger']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limbic.emotion.models.tf_limbic_model import utils\n",
    "\n",
    "import pickle\n",
    "\n",
    "SENTENCE_EMOTIONS_TRAIN_FILE = '../data/sentence_emotions_train.pickle'\n",
    "CONTINUES_TO_BINARY_THRESHOLD = 0.5\n",
    "\n",
    "VERSION = '2019-11-16'\n",
    "\n",
    "metadata_file = f'model_metadata_{VERSION}.txt'\n",
    "tokenizer_file = f'tokenizer_{VERSION}.pickle'\n",
    "\n",
    "with open(tokenizer_file, 'rb') as tokenizer_f:\n",
    "    tokenizer = pickle.load(tokenizer_f)\n",
    "\n",
    "\n",
    "def load_data_file(file_path):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    data_sentences = data['text'].str.lower().apply(lambda x: utils.preprocess_sentence(x))\n",
    "    y_data = data[EMOTIONS].values\n",
    "    # This will be used throughout the notebook to compute performance \n",
    "    y_data_labeled = utils.continuous_labels_to_binary(y_data, CONTINUES_TO_BINARY_THRESHOLD)   \n",
    "\n",
    "    # This representation will be needed for sklearn later in this notebook. \n",
    "    x_data = tokenizer.texts_to_sequences(data_sentences)\n",
    "    x_data = tf.keras.preprocessing.sequence.pad_sequences(x_data, maxlen=MAX_LEN)\n",
    "    \n",
    "    return data, x_data, y_data, y_data_labeled, data_sentences\n",
    "\n",
    "test, x_test, y_test, y_test_labeled, test_sentences = load_data_file(SENTENCE_EMOTIONS_TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea5f2b9eb9e47ba97e6cf38b9a0d5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "but i am sure i have always thought of christmas time when it has come roundapart from the veneration due to its sacred name and origin if anything belonging to it can be apart from thatas a good time a kind forgiving charitable pleasant time the only time i know of in the long calendar of the year when men and women seem by one consent to open their shutup hearts freely and to think of people below them as if they really were fellowpassengers to the grave and not another race of creatures bound on other journeys\n",
      "[1 1 1 0]\n",
      "[0.13968161 0.7154621  0.09387286 0.05098342]\n",
      "**********\n",
      "without substantially accomplishing this part of their undertaking they would have very imperfectly fulfilled the object of their appointment or the expectation of the public yet that it could not be easily accomplished will be denied by no one who is unwilling to betray his ignorance of the subject\n",
      "[1 1 0 1]\n",
      "[0.18566225 0.37759364 0.38168907 0.05505502]\n",
      "**********\n",
      "they early introduce us to and detain us in scenery with which otherwise at that age we should have little acquaintance\n",
      "[0 0 0 0]\n",
      "[0.09732424 0.70744586 0.14894189 0.04628792]\n",
      "**********\n",
      "the welshman admitted several ladies and gentlemen among them the widow douglas and noticed that groups of citizens were climbing up the hillto stare at the stile\n",
      "[1 0 0 0]\n",
      "[0.21834868 0.25974202 0.44281492 0.0790944 ]\n",
      "**********\n",
      "he gave me a strange look and then glanced around the room as if he were looking for something that shouldnt be there\n",
      "[0 0 0 0]\n",
      "[0.18948333 0.57819456 0.09929177 0.13303034]\n",
      "**********\n",
      "what are you going to do with those sandy wanted to make certain that his office would be spared\n",
      "[0 0 0 0]\n",
      "[0.14792445 0.6822501  0.09032701 0.07949843]\n",
      "**********\n",
      "thus she went roving on through the wide world and looked neither to the right hand nor to the left nor took any rest for seven years\n",
      "[0 0 0 0]\n",
      "[0.11584835 0.49849707 0.34736848 0.03828603]\n",
      "**********\n",
      "it will be seen in some other place of what a very light substance the entire interior of the sperm whales enormous head consists\n",
      "[0 0 0 0]\n",
      "[0.14445235 0.6468678  0.14129086 0.06738892]\n",
      "**********\n",
      "shire that is a hundred miles off who may she be that sends for people to see her that distance\n",
      "[0 0 0 0]\n",
      "[0.12473074 0.46013328 0.3598017  0.05533431]\n",
      "**********\n",
      "our fate for life was now to be decided\n",
      "[0 0 0 0]\n",
      "[0.16485511 0.63458973 0.08994341 0.11061179]\n",
      "**********\n",
      "she released his shirt and lifted her hands to the back of his head pulling his mouth tighter\n",
      "[0 1 0 0]\n",
      "[0.21759321 0.22196643 0.46115062 0.0992898 ]\n",
      "**********\n",
      "picking it up she puzzled over it\n",
      "[0 0 0 0]\n",
      "[0.20147009 0.17414327 0.5077208  0.11666575]\n",
      "**********\n",
      "zoe go abroad and love a foreign lady\n",
      "[0 1 0 0]\n",
      "[0.1526002  0.70160455 0.0692036  0.07659161]\n",
      "**********\n",
      "kolvaar didnt twist the knife\n",
      "[0 0 0 0]\n",
      "[0.1754265  0.6389002  0.09831848 0.08735479]\n",
      "**********\n",
      "whither away so early little redcap\n",
      "[0 0 0 0]\n",
      "[0.12468245 0.7168518  0.10444051 0.05402528]\n",
      "**********\n",
      "the squall the squall jump my jollies they scatter\n",
      "[0 0 0 0]\n",
      "[0.18096736 0.25054404 0.42506653 0.14342204]\n",
      "**********\n",
      "he wasnt right in the head\n",
      "[0 0 0 0]\n",
      "[0.18737444 0.5799568  0.10272719 0.12994154]\n",
      "**********\n",
      "besides he can summon his wolf and i know not what\n",
      "[0 0 0 0]\n",
      "[0.16498761 0.6507301  0.10944367 0.07483874]\n",
      "**********\n",
      "is a tall man lefthanded limps with the right leg wears thicksoled shootingboots and a grey cloak smokes indian cigars uses a cigarholder and carries a blunt penknife in his pocket\n",
      "[0 0 0 0]\n",
      "[0.17248224 0.46846616 0.30955094 0.0495007 ]\n",
      "**********\n",
      "sect\n",
      "[0 0 0 0]\n",
      "[0.27483234 0.32757753 0.20838757 0.18920253]\n"
     ]
    }
   ],
   "source": [
    "y_pred_bert = []\n",
    "for idx, sentence in tqdm(enumerate(test_sentences[:20])):\n",
    "    print('*' * 10)\n",
    "    print(sentence)\n",
    "    print(y_test_labeled[idx])\n",
    "    p = predict(sentence)\n",
    "    print(p['response']['prediction'].detach().numpy()[0])\n",
    "    y_pred_bert.append(p['response']['prediction'].detach().numpy()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.00      0.00      0.00         3\n",
      "         joy       0.18      0.50      0.27         4\n",
      "        fear       0.00      0.00      0.00         1\n",
      "       anger       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.17      0.22      0.19         9\n",
      "   macro avg       0.05      0.12      0.07         9\n",
      "weighted avg       0.08      0.22      0.12         9\n",
      " samples avg       0.10      0.07      0.07         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_pred_bert_labeled = utils.continuous_labels_to_binary(np.array([list(x) for x in y_pred_bert]), 0.5)\n",
    "\n",
    "print(classification_report(y_test_labeled[:20], y_pred_bert_labeled, target_names=EMOTIONS))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions ML Model Training \n",
    "\n",
    "This notebook is built with the purpose of training and save a deep learning ML model for emotions classification using the sentence emotions dataset from books built using the `Build-emotions-dataset-final` notebook. \n",
    "\n",
    "For more details about the data, please check this notebook.\n",
    "\n",
    "For mode details about the ML model, please check the code. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limbic.emotion.models.tf_limbic_model import utils\n",
    "training_metadata ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (76340, 5)\n",
      "test shape: (19085, 5)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from limbic.limbic_constants import AFFECT_INTENSITY_EMOTIONS as EMOTIONS\n",
    "training_metadata['labels'] = EMOTIONS\n",
    "\n",
    "SENTENCE_EMOTIONS_TEST_FILE = '../data/sentence_emotions_test.pickle' \n",
    "SENTENCE_EMOTIONS_TRAIN_FILE = '../data/sentence_emotions_train.pickle'\n",
    "CONTINUOUS_TO_BINARY_THRESHOLD = 0.5\n",
    "training_metadata['sentence_emotions_test_file'] = SENTENCE_EMOTIONS_TEST_FILE\n",
    "training_metadata['sentence_emotions_train_file'] = SENTENCE_EMOTIONS_TRAIN_FILE\n",
    "training_metadata['continuous_to_binary_threshold'] = CONTINUOUS_TO_BINARY_THRESHOLD\n",
    "\n",
    "VERSION = '2019-11-16'\n",
    "tokenizer_file = f'tokenizer_{VERSION}.pickle'\n",
    "training_metadata['previous_version'] = VERSION\n",
    "\n",
    "# This is the total number of words to be used from the ones found in the corpus (note that 50k is about freq 5)\n",
    "MAX_WORDS = 50000  \n",
    "MAX_LEN = 150  # this is the total number of words allowed in an input sentence of the model (also used for the padding)\n",
    "training_metadata['tokenized_max_words'] = MAX_WORDS\n",
    "training_metadata['tokenized_padding_max_len'] = MAX_LEN\n",
    "\n",
    "\n",
    "with open(tokenizer_file, 'rb') as tokenizer_f:\n",
    "    tokenizer = pickle.load(tokenizer_f)\n",
    "\n",
    "\n",
    "def load_data_file(file_path, tokenizer):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    data_sentences = data['text'].str.lower().apply(lambda x: utils.preprocess_sentence(x))\n",
    "    y_data = data[EMOTIONS].values\n",
    "    # This will be used throughout the notebook to compute performance \n",
    "    y_data_labeled = utils.continuous_labels_to_binary(y_data, CONTINUOUS_TO_BINARY_THRESHOLD)   \n",
    "\n",
    "    # This representation will be needed for sklearn later in this notebook. \n",
    "    x_data = tokenizer.texts_to_sequences(data_sentences)\n",
    "    x_data = tf.keras.preprocessing.sequence.pad_sequences(x_data, maxlen=MAX_LEN)\n",
    "    \n",
    "    return data, x_data, y_data, y_data_labeled, data_sentences\n",
    "\n",
    "\n",
    "train, x_train, y_train, y_train_labeled, train_sentences = load_data_file(SENTENCE_EMOTIONS_TRAIN_FILE, tokenizer)\n",
    "test, x_test, y_test, y_test_labeled, test_sentences = load_data_file(SENTENCE_EMOTIONS_TEST_FILE, tokenizer)\n",
    "\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDING = \"../data/embeddings/glove.6B.100d.txt\"  # available here: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "EMBEDDING_SIZE = 100  # Given the GloVe file used. \n",
    "training_metadata['embeddings_file'] = GLOVE_EMBEDDING\n",
    "training_metadata['embeddings_size'] = EMBEDDING_SIZE\n",
    "\n",
    "embeddings_matrix = utils.build_embeddings_matrix(tokenizer, \n",
    "                                                  max_words=MAX_WORDS, \n",
    "                                                  embeddings_file=GLOVE_EMBEDDING,\n",
    "                                                  embedding_size=EMBEDDING_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 150, 100)     5000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 150, 512)     549888      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 146, 128)     327808      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            1028        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,878,724\n",
      "Trainable params: 878,724\n",
      "Non-trainable params: 5,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# These values were tweaked manually but should be included in a Hyper-parameters search with Bayesian optimization\n",
    "# More info: https://www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/\n",
    "DROP_OUT_RATE = 0.1\n",
    "MODEL_METRICS = ['accuracy']\n",
    "LOSS_FUNCTION = 'binary_crossentropy'\n",
    "ADAM_LR_PARAMETER = 1e-3\n",
    "\n",
    "# TODO: move all params to config.cfg file and load from disk. \n",
    "params = utils.ModelParams(\n",
    "    max_len=MAX_LEN,\n",
    "    max_words=MAX_WORDS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    drop_out_rate=DROP_OUT_RATE,\n",
    "    labels=EMOTIONS,\n",
    "    loss_function=LOSS_FUNCTION,\n",
    "    adam_lr_parameter=ADAM_LR_PARAMETER,\n",
    "    model_metrics=MODEL_METRICS)\n",
    "\n",
    "training_metadata['training_params'] = params._asdict()\n",
    "\n",
    "model = utils.build_model(embeddings_matrix, params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "269/269 [==============================] - 587s 2s/step - loss: 0.3579 - accuracy: 0.3132 - val_loss: 0.2770 - val_accuracy: 0.4720\n",
      "Epoch 2/3\n",
      "269/269 [==============================] - 610s 2s/step - loss: 0.2714 - accuracy: 0.4248 - val_loss: 0.2565 - val_accuracy: 0.4095\n",
      "Epoch 3/3\n",
      "269/269 [==============================] - 596s 2s/step - loss: 0.2519 - accuracy: 0.4355 - val_loss: 0.2445 - val_accuracy: 0.4591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152efdcc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EPOCHS = 3  # using low number as example\n",
    "TENSORFLOW_LOGS = './tensorflow_logs'\n",
    "training_metadata['training_batch_size'] = BATCH_SIZE\n",
    "training_metadata['training_validation_split'] = VALIDATION_SPLIT\n",
    "training_metadata['training_epochs'] = EPOCHS\n",
    "training_metadata['training_tensorflow_logs'] = TENSORFLOW_LOGS\n",
    "\n",
    "# Adding a callback to review train/test learning curves in Tensorboard \n",
    "# TODO: Add callbacks to make use of checkpoints if needed. \n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=TENSORFLOW_LOGS)]\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          validation_split=VALIDATION_SPLIT, \n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS, \n",
    "          callbacks=callbacks, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(tokenizer, model, training_metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

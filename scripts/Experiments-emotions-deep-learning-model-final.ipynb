{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models\n",
    "\n",
    "The bi-directional RNN built in previous jupyter notebook will be evaluated against Facebook's FastText and other multi-label classification models based on scikit-learn will be used. The idea is to evaluate both the general classification performance and the ability of the models to generate weighted predictions for each label similar to the ones generated by the lexicon-based emotions model. \n",
    "\n",
    "\n",
    "\n",
    "## Limbic Bi-directional RNN Multilabel Classifier \n",
    "\n",
    "First let's check the performance of such model with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from limbic.emotion.models.tf_limbic_model import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mbert\u001b[m\u001b[m                           model_metadata_2019-11-16.json\r\n",
      "bert_model.bin                 tokenizer_2019-11-16.pickle\r\n",
      "emotions_model_2019-11-16.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../limbic/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '2019-11-16'\n",
    "\n",
    "metadata_file = f'model_metadata_{VERSION}.txt'\n",
    "tokenizer_file = f'tokenizer_{VERSION}.pickle'\n",
    "model_path = '../limbic/models/emotions_model_2019-11-16.h5'\n",
    "\n",
    "with open(tokenizer_file, 'rb') as tokenizer_f:\n",
    "    tokenizer = pickle.load(tokenizer_f)\n",
    "\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limbic.limbic_constants import AFFECT_INTENSITY_EMOTIONS as EMOTIONS\n",
    "from limbic.emotion.models.tf_limbic_model import TfLimbicModel\n",
    "from limbic.limbic_types import ModelParams\n",
    "\n",
    "# These are variables needed to load and use the model\n",
    "MAX_LEN = 150\n",
    "model_params = ModelParams(model=model, tokenizer=tokenizer, max_len=MAX_LEN, emotions=EMOTIONS, specific_params=None)\n",
    "\n",
    "# To create the limbic model from scratch we need to pass down the TensorFlow model, the tokenizer and some paramters\n",
    "tf_model = TfLimbicModel(model_params)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also load the model without passing down any parameter and will use the latest configured in the code base, as simply as the following (check also the `ML-model-example` notebook):\n",
    "\n",
    "```python\n",
    "tf_model = TfLimbicModel()\n",
    "```\n",
    "\n",
    "Given that in this notebook we are indeed computing the performance for a very specific model, I'm allowing to generate such models by passing some of the parameters used in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (76340, 5)\n",
      "test shape: (19085, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "SENTENCE_EMOTIONS_TEST_FILE = '../data/sentence_emotions_test.pickle' \n",
    "SENTENCE_EMOTIONS_TRAIN_FILE = '../data/sentence_emotions_train.pickle'\n",
    "CONTINUES_TO_BINARY_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "def load_data_file(file_path):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    data_sentences = data['text'].str.lower().apply(lambda x: utils.preprocess_sentence(x))\n",
    "    y_data = data[EMOTIONS].values\n",
    "    # This will be used throughout the notebook to compute performance \n",
    "    y_data_labeled = utils.continuous_labels_to_binary(y_data, CONTINUES_TO_BINARY_THRESHOLD)   \n",
    "\n",
    "    # This representation will be needed for sklearn later in this notebook. \n",
    "    x_data = tokenizer.texts_to_sequences(data_sentences)\n",
    "    x_data = tf.keras.preprocessing.sequence.pad_sequences(x_data, maxlen=MAX_LEN)\n",
    "    \n",
    "    return data, x_data, y_data, y_data_labeled, data_sentences\n",
    "\n",
    "\n",
    "train, x_train, y_train, y_train_labeled, train_sentences = load_data_file(SENTENCE_EMOTIONS_TRAIN_FILE)\n",
    "test, x_test, y_test, y_test_labeled, test_sentences = load_data_file(SENTENCE_EMOTIONS_TEST_FILE)\n",
    "\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9faac6c6144c7087e45cb387a5743b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from limbic.emotion.models.tf_limbic_model import TfLimbicModel\n",
    "\n",
    "tf_model = TfLimbicModel()  \n",
    "\n",
    "y_pred_tf = []\n",
    "for sentence in tqdm(test_sentences):\n",
    "    prediction = tf_model.predict(sentence)\n",
    "    y_pred_tf.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.82      0.42      0.56      2430\n",
      "         joy       0.77      0.54      0.63      3603\n",
      "        fear       0.81      0.49      0.61      2684\n",
      "       anger       0.71      0.47      0.57      1783\n",
      "\n",
      "   micro avg       0.78      0.49      0.60     10500\n",
      "   macro avg       0.78      0.48      0.59     10500\n",
      "weighted avg       0.78      0.49      0.60     10500\n",
      " samples avg       0.18      0.16      0.16     10500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_tf_labeled = utils.continuous_labels_to_binary(np.array([list(x) for x in y_pred_tf]), 0.5)\n",
    "print(classification_report(y_test_labeled, y_pred_tf_labeled, target_names=EMOTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText MultiLabel Classifier\n",
    "\n",
    "I'm skipping fasttext experiment for now as it's latest version has some dependencies incompatible with pytorch and tensorflow dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# These methods are exclusively for transforming the train and test datasets to fasttext format. \n",
    "# \"\"\"\n",
    "\n",
    "# def add_label(key, value):    \n",
    "#     return f'__label__{key}' if value > 0 else None\n",
    "\n",
    "# def prepare_for_fasttext(data, suffix=''):\n",
    "#     \"\"\"\n",
    "#     This will generate the dataset needed for fasttext, each line will be something like the following:\n",
    "#     __label__joy this is joy\\n\n",
    "    \n",
    "#     Each sentence is preprocessed using the preprocess_sentence method used in the deep learning model above. \n",
    "#     \"\"\"\n",
    "#     with open(f'../data/fasttext_{suffix}', 'w') as f:\n",
    "#         for index, row in tqdm(data.iterrows(), f'iterating data {suffix}'):\n",
    "#             labels = ' '.join([x for x in [add_label(emotion, row[emotion]) for emotion in EMOTIONS] if x])\n",
    "#             f.write(f\"{labels} {utils.preprocess_sentence(row['text'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_for_fasttext(train, 'train')\n",
    "# prepare_for_fasttext(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "\n",
    "# ft_model = fasttext.train_supervised(\n",
    "#     input=\"../data/fasttext_train\", \n",
    "#     lr=0.5, \n",
    "#     epoch=25, \n",
    "#     wordNgrams=2, \n",
    "#     bucket=200000, \n",
    "#     dim=100, \n",
    "#     loss='ova')  # One vs All strategy for training a multi-label classification model\n",
    "\n",
    "# # TODO: consider adding FastText model to have a \"fast\" version of the classifier \n",
    "# # (it's orders of magnitude faster than the others but not very accurate)\n",
    "# ft_model.save_model(\"model_fasttext.bin\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fasttext_prediction(ft_model, sentence):\n",
    "#     p = ft_model.predict(sentence, k=-1)\n",
    "#     return {k.split('__')[-1]:min(1.0, max(v, 0.0001)) for k, v in zip(*p)}\n",
    "\n",
    "\n",
    "# def np_fasttext_prediction(ft_model, sentence, categories):\n",
    "#     label_p = fasttext_prediction(ft_model, sentence)\n",
    "#     return np.array([label_p[c] for c in categories])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext_prediction(ft_model, 'i have joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# y_pred_ft = []\n",
    "# for sentence in tqdm(test_sentences):\n",
    "#     prediction = np_fasttext_prediction(ft_model, sentence, EMOTIONS)\n",
    "#     y_pred_ft.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_labeled_ft = utils.continuous_labels_to_binary(np.array([list(x) for x in y_pred_ft]), CONTINUES_TO_BINARY_THRESHOLD)\n",
    "# print(classification_report(y_test_labeled, y_pred_labeled_ft, target_names=EMOTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-MultiLearn RandomForest using BinaryRelevance (One-vs-All)\n",
    "\n",
    "For more details on scikit-multilearn you should check this post: https://xang1234.github.io/multi-label/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDING = \"../data/embeddings/glove.6B.100d.txt\"\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_WORDS = 50000\n",
    "\n",
    "# TODO: explore building a new method to return a dictionary instead of a list as lookup \n",
    "# operations are much faster (re-using the one needed by TensorFlow for the moment)\n",
    "embeddings_matrix = utils.build_embeddings_matrix(tokenizer, \n",
    "                                                  max_words=MAX_WORDS, \n",
    "                                                  embeddings_file=GLOVE_EMBEDDING,\n",
    "                                                  embedding_size=EMBEDDING_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Very simple embeddings combination strategy (using the average). \n",
    "    \n",
    "    TODO: Some other strategies could be tested. \n",
    "    \"\"\"\n",
    "    avg_emb = np.zeros((1, len(embeddings[0])), dtype='float32')[0]\n",
    "    for e in embeddings:\n",
    "        for idx, eb in enumerate(e):\n",
    "            avg_emb[idx] += eb\n",
    "    return avg_emb / len(embeddings)\n",
    "\n",
    "\n",
    "def prepare_for_sklearn(x_data):    \n",
    "    \"\"\"\n",
    "    basically transform the input dataset to a dataset of embeddings. \n",
    "    \"\"\"\n",
    "    x_data_sk = []\n",
    "    for xt in tqdm(x_data):\n",
    "        embs = [embeddings_matrix[_v] for _v in xt]\n",
    "        x_data_sk.append(combine_embeddings(embs))\n",
    "    return x_data_sk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1de91902ed4be49c604f6527f3a351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_sklearn = prepare_for_sklearn(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given that these models are quite slow to train, I can't use the +75k objects (at least when running this in my laptop). \n",
    "\n",
    "Will do some simple sampling and use a smaller dataset for training scikit-learn based models. \n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_db(x, y, n=10000):\n",
    "    s = random.sample(range(len(x)), n)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    for idx in s:\n",
    "        new_x.append(x[idx])\n",
    "        new_y.append(y[idx])\n",
    "    return np.array(new_x), np.array(new_y)\n",
    "\n",
    "x_train_sample, y_train_sample = sample_db(x_train_sklearn, y_train, n=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=RandomForestClassifier(bootstrap=True,\n",
       "                                                  class_weight=None,\n",
       "                                                  criterion='gini', max_depth=5,\n",
       "                                                  max_features='auto',\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=10,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=50, n_jobs=None,\n",
       "                                                  oob_score=False,\n",
       "                                                  random_state=42, verbose=0,\n",
       "                                                  warm_start=False),\n",
       "                require_dense=[False, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "def prepare_y(y):\n",
    "    \"\"\"\n",
    "    If there's any value predicted for a category above CONTINUES_TO_BINARY_THRESHOLD, \n",
    "    we'll consider the category present.\n",
    "    \n",
    "    TODO: There must be a method that does this in sklearn but doing this manually anyways.\n",
    "    \"\"\"\n",
    "    new_y = np.zeros(y.shape, dtype='float32')\n",
    "    for iidx, i in enumerate(y):\n",
    "        for jidx, j in enumerate(i):\n",
    "            if j > 0.1:\n",
    "                new_y[iidx][jidx] = 1\n",
    "    return new_y\n",
    "            \n",
    "# TODO: Tweak and do a proper parameter tuning for this model\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier(n_estimators=50, max_depth=5, \n",
    "                                        min_samples_split=10, random_state=42),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "classifier.fit(x_train_sklearn, prepare_y(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaceb8d4c5e7438ba61691287e9dd3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_sklearn = prepare_for_sklearn(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df44e6c7a094f6497e44606a3c2c90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scipy-1.2.1-py3.7-macosx-10.14-x86_64.egg/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scipy-1.2.1-py3.7-macosx-10.14-x86_64.egg/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    }
   ],
   "source": [
    "def mlpredict(x):\n",
    "    prediction = classifier.predict_proba(np.array([x]))[0]\n",
    "    index = prediction[0].rows[0]\n",
    "    preds = []\n",
    "    for idx in range(4):\n",
    "        if idx in index:\n",
    "            pos = index.index(idx)\n",
    "            preds.append(prediction[0].data[0][pos])\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    return preds \n",
    "\n",
    "y_pred_skml = []\n",
    "for _x in tqdm(x_test_sklearn):\n",
    "    y_pred_skml.append(mlpredict(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.43      0.13      0.20      2430\n",
      "         joy       0.41      0.44      0.43      3603\n",
      "        fear       0.43      0.17      0.25      2684\n",
      "       anger       0.41      0.07      0.12      1783\n",
      "\n",
      "   micro avg       0.42      0.24      0.30     10500\n",
      "   macro avg       0.42      0.21      0.25     10500\n",
      "weighted avg       0.42      0.24      0.28     10500\n",
      " samples avg       0.08      0.08      0.07     10500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_labeled_skml = utils.continuous_labels_to_binary(np.array([list(x) for x in y_pred_skml]), CONTINUES_TO_BINARY_THRESHOLD)\n",
    "print(classification_report(y_test_labeled, y_pred_labeled_skml, target_names=EMOTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn One vs All SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=100, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma=0.5, kernel='rbf',\n",
       "                                  max_iter=-1, probability=True,\n",
       "                                  random_state=42, shrinking=True, tol=0.001,\n",
       "                                  verbose=1),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm_classifier = OneVsRestClassifier(\n",
    "    SVC(kernel='rbf', gamma=0.5, C=100, verbose=1, probability=True, random_state=42))\n",
    "svm_classifier.fit(x_train_sample, prepare_y(y_train_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sklearn = svm_classifier.predict_proba(x_test_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.50      0.41      0.45      2430\n",
      "         joy       0.47      0.66      0.54      3603\n",
      "        fear       0.55      0.47      0.50      2684\n",
      "       anger       0.50      0.29      0.37      1783\n",
      "\n",
      "   micro avg       0.49      0.49      0.49     10500\n",
      "   macro avg       0.50      0.46      0.47     10500\n",
      "weighted avg       0.50      0.49      0.48     10500\n",
      " samples avg       0.15      0.16      0.15     10500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/gastonlhuillier/Personal/limbic/.python/limbic/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-macosx-10.14-x86_64.egg/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_labeled_sklearn = utils.continuous_labels_to_binary(np.array([list(x) for x in y_pred_sklearn]), CONTINUES_TO_BINARY_THRESHOLD)\n",
    "print(classification_report(y_test_labeled, y_pred_labeled_sklearn, target_names=EMOTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

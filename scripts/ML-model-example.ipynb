{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ML model for Emotion Analysis\n",
    "\n",
    "Similar to the notebook for lexicon-based model, this is just a quick walkthrough to understand how to load and use the ML model pre-built in the `limbic` package.\n",
    "\n",
    "\n",
    "First, you need to understand the constraints and limitations of the model:\n",
    "- It was built only for a very narrow set of emotions (called Affection Emotions in `limbic`), which are \"joy\", \"sadness\", \"anger\", and \"fear\".\n",
    "- It was built with a synthetic dataset created using the lexicon-based model from a very particular dataset (top ~90 books from different websites). This means that any biases that could come from this setup will be included in the trained model.\n",
    "- Emotions were not computed using any context disambiguation for the books used, as shown in the Game of Thrones example, it's important that you setup some context when computing emotions (words could mean totally different things depending on the context), so any unfortunate relationship associated by the lexicon-based model could be included in the ML model.\n",
    "- Parameters for the ML model were not tweaked with the full extend of hyper-parameter search parameters (given that I fit the models in my current laptop), which means that it might not be the best version of itself. Same goes for the benchmark experiments with other models (FastText and Scikilearn-based models). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limbic.emotion.models.tf_limbic_model import TfLimbicModel\n",
    "\n",
    "tf_model = TfLimbicModel()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53164005, 0.9596189 , 0.1189495 , 0.05770493], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.predict('I have a lot of joy and sadness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmotionValue(category='sadness', value=0.53164005),\n",
       " EmotionValue(category='joy', value=0.9596189),\n",
       " EmotionValue(category='fear', value=0.1189495),\n",
       " EmotionValue(category='anger', value=0.057704926)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.get_sentence_emotions('I have a lot of joy and sadness.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works within the boundaries for a full sentence. If the sentence is larger than 150 words, then it will be clipped to the first 150 words, and it doesn't consider special relationship for the context of the sentence. \n",
    "\n",
    "Another special difference between this model and the lexicon-based, is that in this case the negation has not been properly learned. This is for sure one of the main areas of improvement for this model (e.g. \"I'm not happy\" will be classified with a high likelihood of \"joy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmotionValue(category='sadness', value=0.008989334),\n",
       " EmotionValue(category='joy', value=0.5965177),\n",
       " EmotionValue(category='fear', value=0.0073697567),\n",
       " EmotionValue(category='anger', value=0.012596458)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.get_sentence_emotions('i am not happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be considered by anyone before using this model for any analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
